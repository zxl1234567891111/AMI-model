{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7200582d-a2db-48a3-bb62-74a6cce09d34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------- RandomForest -------------------\n",
      "Optimal feature selection scheme (predefined):\n",
      "Number of selected features: 11\n",
      "Selected features: ['sex', 'Hbp', 'CVD', 'CVD_inheritance', 'MAP', 'LDL-C', 'K', 'NT_proBNP', 'cTnI', 'CK-MB', 'MYO']\n",
      "\n",
      "------------------- SVM -------------------\n",
      "Optimal feature selection scheme (predefined):\n",
      "Number of selected features: 8\n",
      "Selected features: ['sex', 'CVD', 'Diabetes', 'CVD_inheritance', 'LDL-C', 'glucose', 'NT_proBNP', 'MYO']\n",
      "\n",
      "------------------- XGBoost -------------------\n",
      "Optimal feature selection scheme (predefined):\n",
      "Number of selected features: 12\n",
      "Selected features: ['sex', 'Hbp', 'CVD', 'heart_rate_odd', 'CVD_inheritance', 'smoking_index', 'glucose', 'K', 'NT_proBNP', 'cTnI', 'CK-MB', 'MYO']\n",
      "\n",
      "------------------- LightGBM -------------------\n",
      "Optimal feature selection scheme (predefined):\n",
      "Number of selected features: 11\n",
      "Selected features: ['age', 'BMI', 'Ki_level', 'heart_rate_odd', 'CVD_inheritance', 'smoking_index', 'MAP', 'NT_proBNP', 'cTnI', 'CK-MB', 'MYO']\n",
      "\n",
      "------------------- KNN -------------------\n",
      "Optimal feature selection scheme (predefined):\n",
      "Number of selected features: 13\n",
      "Selected features: ['sex', 'BMI', 'Ki_level', 'Hbp', 'CVD', 'Diabetes', 'cerebrovascular', 'MAP', 'K', 'NT_proBNP', 'cTnI', 'CK-MB', 'MYO']\n",
      "\n",
      "------------------- MLP -------------------\n",
      "Optimal feature selection scheme (predefined):\n",
      "Number of selected features: 10\n",
      "Selected features: ['sex', 'CVD', 'smoking_index', 'MAP', 'LDL-C', 'glucose', 'NT_proBNP', 'cTnI', 'CK-MB', 'MYO']\n",
      "\n",
      "------------------- LogisticRegression -------------------\n",
      "Optimal feature selection scheme (predefined):\n",
      "Number of selected features: 13\n",
      "Selected features: ['age', 'BMI', 'Ki_level', 'CVD', 'cerebrovascular', 'CVD_inheritance', 'MAP', 'glucose', 'K', 'NT_proBNP', 'cTnI', 'CK-MB', 'MYO']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.metrics import f1_score\n",
    "from deap import base, creator, tools, algorithms\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb  # Import XGBoost\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression  # Import Logistic Regression model\n",
    "import lightgbm as lgb  # Import LightGBM\n",
    "from sklearn.neighbors import KNeighborsClassifier  # Import KNN model\n",
    "from sklearn.neural_network import MLPClassifier  # Import MLPClassifier\n",
    "\n",
    "# Set random seed\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# ------------------- Step 1: Load Data -------------------\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_excel(r'AMI-DATA-imputed.xlsx')\n",
    "\n",
    "# Drop rows where 'labels' is missing\n",
    "data = data.dropna(subset=['labels'])\n",
    "\n",
    "# Ensure all columns are numeric\n",
    "data = data.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Separate features and labels\n",
    "X = data.drop(columns=['labels'])\n",
    "y = data['labels']\n",
    "\n",
    "# ------------------- Step 3: Define Genetic Algorithm for Feature Selection -------------------\n",
    "class FeatureSelectionGA:\n",
    "    def __init__(self, X, y, model_name, num_folds=3, population_size=20, generations=10, cx_prob=0.8, mut_prob=0.05):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.num_folds = num_folds\n",
    "        self.population_size = population_size\n",
    "        self.generations = generations\n",
    "        self.cx_prob = cx_prob\n",
    "        self.mut_prob = mut_prob\n",
    "        self.kfold = KFold(n_splits=self.num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "        # Select model based on model name\n",
    "        if model_name == 'XGBoost':\n",
    "            self.model = xgb.XGBClassifier(n_estimators=100, random_state=42)\n",
    "        elif model_name == 'SVM':\n",
    "            self.model = SVC(kernel='rbf', gamma=0.01, random_state=42)\n",
    "        elif model_name == 'RandomForest':\n",
    "            self.model = RandomForestClassifier(n_estimators=50, random_state=42)\n",
    "        elif model_name == 'LogisticRegression':\n",
    "            self.model = LogisticRegression(max_iter=5000, random_state=42)  # Set maximum iterations to ensure convergence\n",
    "        elif model_name == 'LightGBM':\n",
    "            self.model = lgb.LGBMClassifier(n_estimators=100, random_state=42)\n",
    "        elif model_name == 'KNN':\n",
    "            self.model = KNeighborsClassifier(n_neighbors=5)  # Default to 5 neighbors\n",
    "        elif model_name == 'MLP':\n",
    "            self.model = MLPClassifier(hidden_layer_sizes=(10,), max_iter=1000, random_state=42)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid model name\")\n",
    "\n",
    "        # Define DEAP fitness and individual\n",
    "        creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n",
    "        creator.create(\"Individual\", list, fitness=creator.FitnessMax)\n",
    "\n",
    "        self.toolbox = base.Toolbox()\n",
    "        self.toolbox.register(\"attr_bool\", random.randint, 0, 1)\n",
    "        self.toolbox.register(\"individual\", tools.initRepeat, creator.Individual, self.toolbox.attr_bool, n=len(X.columns))\n",
    "        self.toolbox.register(\"population\", tools.initRepeat, list, self.toolbox.individual)\n",
    "\n",
    "        # Selection, crossover, mutation operations\n",
    "        self.toolbox.register(\"mate\", tools.cxTwoPoint)\n",
    "        self.toolbox.register(\"mutate\", tools.mutFlipBit, indpb=1.0/len(X.columns))\n",
    "        self.toolbox.register(\"select\", tools.selTournament, tournsize=3)\n",
    "        self.toolbox.register(\"evaluate\", self.evaluate)\n",
    "\n",
    "    def evaluate(self, individual):\n",
    "        \"\"\"Calculate the classification F1-score for the current feature selection scheme\"\"\"\n",
    "        selected_features = [feature for feature, bit in zip(self.X.columns, individual) if bit == 1]\n",
    "        if len(selected_features) == 0:\n",
    "            return (0,)  # Avoid the case with no features\n",
    "\n",
    "        X_selected = self.X[selected_features]\n",
    "\n",
    "        scores = cross_val_score(self.model, X_selected, self.y, cv=self.kfold, scoring='f1')\n",
    "        return (scores.mean(),)  # Fitness is the F1-score\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"Run the genetic algorithm\"\"\"\n",
    "        population = self.toolbox.population(n=self.population_size)\n",
    "        hof = tools.HallOfFame(1)  # Record the best solution\n",
    "        stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
    "        stats.register(\"max\", np.max)\n",
    "        stats.register(\"avg\", np.mean)\n",
    "\n",
    "        population, logbook = algorithms.eaSimple(population, self.toolbox, cxpb=self.cx_prob, mutpb=self.mut_prob,\n",
    "                                                  ngen=self.generations, stats=stats, halloffame=hof, verbose=True)\n",
    "\n",
    "        best_solution = hof[0]\n",
    "        best_features = [feature for feature, bit in zip(self.X.columns, best_solution) if bit == 1]\n",
    "\n",
    "        print(\"\\nOptimal feature selection scheme (predefined):\")\n",
    "        print(f\"Number of selected features: {len(best_features)}\")\n",
    "        print(f\"Selected features: {best_features}\")\n",
    "\n",
    "        return best_features\n",
    "\n",
    "# Test code\n",
    "models = ['XGBoost', 'SVM', 'RandomForest', 'LogisticRegression', 'LightGBM', 'KNN', 'MLP']\n",
    "for model_name in models:\n",
    "    print(f\"------------------- {model_name} -------------------\")\n",
    "    ga = FeatureSelectionGA(X, y, model_name)\n",
    "    ga.run()\n",
    "    # Delete the created classes\n",
    "    if 'FitnessMax' in creator.__dict__:\n",
    "        del creator.__dict__['FitnessMax']\n",
    "    if 'Individual' in creator.__dict__:\n",
    "        del creator.__dict__['Individual']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:test_1]",
   "language": "python",
   "name": "conda-env-test_1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
